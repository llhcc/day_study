<2016/3/28>
一、
Spring Batch的架构

一个Batch Job是指一系列有序的Step的集合，它们作为预定义流程的一部分而被执行；

Step代表一个自定义的工作单元，它是Job的主要构件块；每一个Step由三部分组成：ItemReader、ItemProcessor、ItemWriter；这三个部分将执行在每一条被处理的记录上，
ItemReader读取每一条记录，然后传递给ItemProcessor处理，最后交给ItemWriter做持久化；ItemProcessor不是必须的，
一个Step可以仅仅包含ItemReader和ItemWriter；如果你不需要去读写任何数据，你可以仅仅在一个Step中包含一个Tasklet（等价于ItemProcessor）；

组成Spring Batch的一些相关的类和接口：

org.springframework.batch.core.Job：表示一个Job，同时也提供了执行Job的能力；
org.springframework.batch.core.Step：表示一个step，同时也提供了执行Step的能力；
org.springframework.batch.item.ItemReader<T>：提供了读取数据的能力；
org.springframework.batch.item.ItemProcessor<T>：我们可以通过它应用业务逻辑到每一条要处理的数据；
org.springframework.batch.item.ItemWriter<T>：提供了写数据的能力
Spring Batch通过这种方式构建一个Job的优点在于解耦每一个Step到它自己独立的处理器当中；每一个Step负责得到数据、应用业务逻辑到这些数据、写数据到适当的位置；




</2016/3/28>
<2016/3/30>
localStorage、sessionStorage用法总结
巧用sessionStorage判断页面是关闭还是刷新

</2016/3/30>
<2016/4/5>
一、prototype（http://www.360doc.com/content/15/0313/15/18624365_454836336.shtml）
1.struts2为每个线程提供一个action实例,多线程访问时不会出现问题。当使用spring管理struts2的action实例对象时,scope必须配置为prototype或者session,若配置为singleton则多线程访问时会出现问题,例如actionMessage,fieldError等信息会累加,多用户访问时有的用户访问到的是另一个用户的数据。

2.scope=“prototype”是为每个请求提供一个action实例(与struts2的机制是一样的)。 
scope=“session”是为每个会话提供一个action实例。 

3.通常使用prototype,即让spring容器为每个请求提供一个action实例,好处是服务器端不用维护用户状态信息,否则使用session服务器端必须存储状态信息,用户多时占用服务器端内存过多。使用prototype时,必须自己在客户端维护用户的状态,每次访问服务端时将相应状态信息提交给服务器。

例如scope=“prototype”时,页面一般< input name="id" type="hidden" value="${id}"/>用来存储用户的id信息,访问action时提交到server端供action中函数使用。而使用scope=“session”时,页面不必使用hidden的对象隐藏id信息,只要服务端获取过用户的id,action中的id属性即会保存这个信息。
 
以下是个人感触：
如上类似的问题，也要注意，其他service如果是singleton，需要注意多线程时会不会出问题，还有，虽然action是单独的对象，可以说是线程安全的，但是，在action中调用其他的关键资源时，也要注意是否需要同步访问，配置SSH框架一定要根据需求多做调整。




</2016/4/5>
<2016/4/13>
一、json在线解析
http://www.bejson.com/

二、Error 500: Filter [struts2]: could not be initialized
was 6.1 ，structs 2的应用部署时出现
 Error 500: Filter [struts2]: could not be initialized 

检查WEB-INF/lib目录的下的jar是否有重复，剔除重复jar文件后重启。

初始化struts失败，重启

</2016/4/13>
<2016/4/20>
可以使用crontab -l命令列出文件的全部信息，crontab -l显示的内容将包括的文件路径；
示例：
	$ crontab -l 
   	# (crondave installed on Tue May 4 13:07:43 1999)
   	# DT:ech the date to the console every 30 minites
  	0,15,30,45 18-06 * * * /bin/echo `date` > /dev/tty1
   	# DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month
   	30 3 1,7,14,21,26 * * /bin/find -name "core' -exec rm {} \;

</2016/4/20>
<2016/4/25>
数据库表日志，使用触发器实现。
table和table_log
</2016/4/25>
<2016/4/29>
多线程webservice


</2016/4/29>
curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{
  "fromStation": "HFH",
  "toStation": "WHN",
  "trainDate": "2016-07-01"
}' 'http://10.10.39.103/router/service/cr/trains'


http://code.alibabatech.com/schema/dubbo/dubbo.xsd
window-->preferences-->xml-->xml catalog-->Add -->
<2016/6/11>
Tomcat 7下如何利用 catalina.properties 部署公用类
Tomcat 有很多配置文件，其中一个是  catalina.properties ，本文介绍catalina.properties 中的设置项。
一、组成
catalina.properties中的设置项包括四个部分：
＃第一部分：安全设置
package.access
package.definition
＃第二部分：类加载设置
common.loader
server.loader
shared.loader
＃第三部分：不需要扫描的类设置
tomcat.util.scan.DefaultJarScanner.jarsToSkip
org.apache.catalina.startup.ContextConfig.jarsToSkip
org.apache.catalina.startup.TldConfig.jarsToSkip
＃第四部分：字符缓存设置
tomcat.util.buf.StringCache.byte.enabled
tomcat.util.buf.StringCache.char.enabled
tomcat.util.buf.StringCache.trainThreshold
tomcat.util.buf.StringCache.cacheSize
二、安全设置
请参见 ：
http://tomcat.apache.org/tomcat-7.0-doc/security-manager-howto.html
http://www.oracle.com/technetwork/Java/seccodeguide-139067.html
三、类加载设置
请参见：
http://tomcat.apache.org/tomcat-7.0-doc/class-loader-howto.html
1、classloader加载顺序
Bootstrap--->System--->/WEB-INF/classes---> /WEB-INF/lib/*.jar---> Common
Common的配置是通过 catalina.properties中的common.loader设置的。
2、common.loader设置
通常情况下，common.loader是已经设置好的，不需要修改。
common.loader包括以下路径：
unpacked classes and resources in $CATALINA_BASE/lib
JAR files in $CATALINA_BASE/lib
unpacked classes and resources in $CATALINA_HOME/lib
JAR files in $CATALINA_HOME/lib
和默认的一些jar包：
annotations-api.jar ― JavaEE annotations classes.
catalina.jar ― Implementation of the Catalina servlet container portion of Tomcat.
catalina-ant.jar ― Tomcat Catalina Ant tasks.
catalina-ha.jar ― High availability package.
catalina-tribes.jar ― Group communication package.
ecj-*.jar ― Eclipse JDT Java compiler.
el-api.jar ― EL 2.2 API.
jasper.jar ― Tomcat Jasper JSP Compiler and Runtime.
jasper-el.jar ― Tomcat Jasper EL implementation.
jsp-api.jar ― JSP 2.2 API.
servlet-api.jar ― Servlet 3.0 API.
tomcat-api.jar ― Several interfaces defined by Tomcat.
tomcat-coyote.jar ― Tomcat connectors and utility classes.
tomcat-dbcp.jar ― Database connection pool implementation based on package-renamed copy of Apache Commons Pool and Apache Commons DBCP.
tomcat-i18n-**.jar ― Optional JARs containing resource bundles for other languages. As default bundles are also included in each individual JAR, they can be safely removed if no internationalization of messages is needed.
tomcat-jdbc.jar ― An alternative database connection pool implementation, known as Tomcat JDBC pool. See documentation for more details.
tomcat-util.jar ― Common classes used by various components of Apache Tomcat.
这些都是系统和工具类，比如数据库的驱动类库、log 类库可以放到此处，web应用的jar 不要放到common.loader 中。
3、server.loader 和 shared.loader
在common.loader 加载完后，tomcat启动程序会检查 catalina.properties文件中配置的server.loader和shared.loader是否设置
如果设置，读取 tomcat下对应的server和shared这两个目录的类库。
server和shared是对应tomcat目录下的两个目录,默认的Tomcat7 下这两个目录是没有的。
设置方法：
server.loader=${catalina.base}/server/classes,${catalina.base}/server/lib/*.jar
shared.loader=${catalina.base}/server/classes,${catalina.base}/server/lib/*.jar
同时需要在tomcat目录下创建 server和shared目录结构并将公用的、应用类放到里面。
Bootstrap--->System--->/WEB-INF/classes---> /WEB-INF/lib/*.jar---> Common--->Server--->Shared
四、忽略扫描的类设置
默认的一些类tomcat已经设置。
五、字符缓存设置
根据需要设置字符的缓存策略。
默认 tomcat.util.buf.StringCache.byte.enabled=true 设置，其他的被注释了。
六、小结

Tomcat可以通过catalina.properties的server和shared设置，为webapp提供公用类库。

使一些公用的、不需要与webapp放在一起的设置信息单独保存，在更新webapp的war的时候无需更改webapp的设置。
</2016/6/11>
<2016/6/13>
本地工程
<dependency>
			<groupId>servlet-api</groupId>
			<artifactId>servlet-api</artifactId>
			<version>1.0</version>
			<scope>system</scope>
			<systemPath>${project.basedir}/src/main/webapp/WEB-INF/lib/servlet-api.jar</systemPath>
		</dependency>	


</2016/6/13>
<>
Value '0000-00-00 00:00:00' can not be represented as java.sql.Timestamp
给jdbc   url加上   zeroDateTimeBehavior参数：

datasource.url=jdbc:mysql://localhost:3306/pe useUnicode=true&characterEncoding=gbk&zeroDateTimeBehavior=convertToNull


zeroDateTimeBehavior=round是为了指定MySql中的DateTime字段默认值查询时的处理方式；默认是抛出异常，

对于值为0000-00-00   00:00:00（默认值）的纪录，如下两种配置，会返回不同的结果：

zeroDateTimeBehavior=round   0001-01-01   00:00:00.0

zeroDateTimeBehavior=convertToNull   null
mysql的datetime类型支持的范围为'1000-01-01'到'9999-12-31'，当你输入不合法的日期或者0自动转为0000-00-00 00:00:00，而这个日期 明显不在我上面提到的范围，所以报错了
timestamp类型取值范围：1970-01-01 00:00:00 到 2037-12-31 23:59:59，报错原理同上

问题：connection holder is null



maven打包命令，不打测试代码
package -Dmaven.test.skip=true

JDK-1.7序列化问题
/**
     * prevent default deserialization
     */
    private void readObject(ObjectInputStream in) throws IOException,
        ClassNotFoundException {
        throw new InvalidObjectException("can't deserialize enum");
    }

    private void readObjectNoData() throws ObjectStreamException {
        throw new InvalidObjectException("can't deserialize enum");
    }


Dubbo管理页面

本地工程调用jar包里某个类的属性为protected时，调用不了则直接在本地工程创建一个相同的类（包路径一致）

dubbo.xsd
preferences--XML--XML Catalog --Add --Location(E:\dubbo.xsd),key type(Namespace name),key(http://code.alibabatech.com/schema/dubbo/dubbo.xsd)
代理服务器：
e proxy server

acegi 

使用kafka的理由：

1.分布式，高吞吐量，速度快（kafka是直接通过磁盘存储，线性读写，速度快：避免了数据在JVM内存和系统内存之间的复制，减少耗性能的对象创建和垃圾回收）

2.同时支持实时和离线两种解决方案（相信很多项目都有类似的需求，这也是Linkedin的官方架构，我们是一部分数据通过storm做实时计算处理，一部分到hadoop做离线分析）。

3.open source (open source 谁不喜欢呢)

4.源码由scala编写，可以运行在JVM上（笔者对scala很有好感，函数式语言一直都挺帅的，spark也是由scala写的，看来以后有空得刷刷scala）

使用场景：

笔者主要是用来做日志分析系统，其实Linkedin也是这么用的，可能是因为kafka对可靠性要求不是特别高，除了日志，网站的一些浏览数据应该也适用。（只要原始数据不需要直接存DB的都可以）

<2016-10-09>
web.xml使用<resource-ref>配置数据源

配置属性注入：
<context:property-placeholder location="classpath:redis.properties" /> 这两个有冲突
<bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">  
        <property name="locations">  
           <list>
              <value>classpath:conf/kafka.properties</value>
              <!-- <value>classpath:redis.properties</value> -->
            </list>  
        </property>  
    </bean>
在spring里使用org.mybatis.spring.mapper.MapperScannerConfigurer 进行自动扫描的时候，设置了sqlSessionFactory 的话，
可能会导致PropertyPlaceholderConfigurer失效，也就是用${jdbc.username}这样之类的表达式，将无法获取到properties文件里的内容。 
导致这一原因是因为，MapperScannerConigurer实际是在解析加载bean定义阶段的，这个时候要是设置sqlSessionFactory的话，会导致提前初始化一些类，
这个时候，PropertyPlaceholderConfigurer还没来得及替换定义中的变量，导致把表达式当作字符串复制了。 但如果不设置sqlSessionFactory 属性的话，
就必须要保证sessionFactory在spring中名称一定要是sqlSessionFactory ，否则就无法自动注入。又或者直接定义 MapperFactoryBean ，再或者放弃自动代理接口方式。 

<bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">  

<property name="basePackage" value="com.xxxx.dal.mapper" /> 

<property name="sqlSessionFactoryBeanName" value="ysSqlSessionFactory" />

 <!-- <property name="sqlSessionFactory" ref="ysSqlSessionFactory"></property> -->

 </bean>

改用sqlSessionFactoryBeanName注入就没有问题(不要使用sqlSessionFactory属性注入，使用sqlSessionFactoryBeanName注入)，
因为这时不会立即初始化sqlSessionFactory,传入的只是名字，非bean，所以不会引发提前初始化问题。。

spring IOC 
资源定位，资源验证解析，资源加载这个才是IoC的核心，理解了getBean与Refresh方法发生的事情才算理解了IoC
IoC与DI

　　首先想说说IoC（Inversion of Control，控制倒转）。这是spring的核心，贯穿始终。所谓IoC，对于spring框架来说，就是由spring来负责控制对象的生命周期和对象间的关系。
这是什么意思呢，举个简单的例子，我们是如何找女朋友的？常见的情况是，我们到处去看哪里有长得漂亮身材又好的mm，
然后打听她们的兴趣爱好、qq号、电话号、ip号、iq号………，想办法认识她们，投其所好送其所要，然后嘿嘿……这个过程是复杂深奥的，我们必须自己设计和面对每个环节。
传统的程序开发也是如此，在一个对象中，如果要使用另外的对象，就必须得到它（自己new一个，或者从JNDI中查询一个），使用完之后还要将对象销毁（比如Connection等），
对象始终会和其他的接口或类藕合起来。

　　那么IoC是如何做的呢？有点像通过婚介找女朋友，在我和女朋友之间引入了一个第三者：婚姻介绍所。婚介管理了很多男男女女的资料，我可以向婚介提出一个列表，
告诉它我想找个什么样的女朋友，比如长得像李嘉欣，身材像林熙雷，唱歌像周杰伦，速度像卡洛斯，技术像齐达内之类的，然后婚介就会按照我们的要求，提供一个mm，
我们只需要去和她谈恋爱、结婚就行了。简单明了，如果婚介给我们的人选不符合要求，我们就会抛出异常。整个过程不再由我自己控制，而是有婚介这样一个类似容器的机构来控制。
Spring所倡导的开发方式就是如此，所有的类都会在spring容器中登记，告诉spring你是个什么东西，你需要什么东西，然后spring会在系统运行到适当的时候，把你要的东西主动给你，
同时也把你交给其他需要你的东西。所有的类的创建、销毁都由 spring来控制，也就是说控制对象生存周期的不再是引用它的对象，而是spring。对于某个具体的对象而言，
以前是它控制其他对象，现在是所有对象都被spring控制，所以这叫控制反转。如果你还不明白的话，我决定放弃。

IoC的一个重点是在系统运行中，动态的向某个对象提供它所需要的其他对象。这一点是通过DI（Dependency Injection，依赖注入）来实现的。
比如对象A需要操作数据库，以前我们总是要在A中自己编写代码来获得一个Connection对象，有了 spring我们就只需要告诉spring，A中需要一个Connection，
至于这个Connection怎么构造，何时构造，A不需要知道。在系统运行时，spring会在适当的时候制造一个Connection，然后像打针一样，注射到A当中，
这样就完成了对各个对象之间关系的控制。A需要依赖 Connection才能正常运行，而这个Connection是由spring注入到A中的，依赖注入的名字就这么来的。那么DI是如何实现的呢？
 Java 1.3之后一个重要特征是反射（reflection），它允许程序在运行的时候动态的生成对象、执行对象的方法、改变对象的属性，
 spring就是通过反射来实现注入的。关于反射的相关资料请查阅java doc。
 
 
</2016-10-09>
<2016-10-10>
为何要领域驱动设计？

简化数据存储
领域驱动设计有很多原因，谈到我为啥要在公司推行领域驱动设计，说起来还是很好玩的，因为原来基于数据驱动的开发方式，也就是传统的多层开发架构，
大家定义了一堆DAL来操作数据, 在.Net大家一般有两种使用方式，一种是用ORM像Entity Framework, 另一种想使用Dapper这样轻量级的Mapping工具，
这些都要把关系型数据转换为对象。结果导致以下几种结果。

没有正确的使用ORM, 导致数据加载过多，导致系统性能很差。
为了解决性能问题，就不加载一些导航属性，但是却把DB Entity返回上层，这样对象的一些属性为空，上层使用这个数据时根本不知道什么时间这个属性是有值的，这个是很丑陋的是不是？
如是又开始使用一些轻量级的数据方法，比如使用Dapper然后自己写SQL语句，这本来是很不错的方式，但是大部分人的SQL能力实在不敢恭维，大部分写出来的SQL语句，
甚至比EnityFramework生成的语句还差。
所以，我就想我们做项目，大部分处理的应该是业务，如何让程序员从数据存储，模型转换的大泥潭里解放出来，领域驱动设计就进入了我的视线，
当然光从数据这个角度还不足以选择领域驱动设计，用一个NoSQL数据库是不是就解决了？ 但是NoSQL也有一些问题，比如MongoDB如何更优雅的保证事务以及数据的一致性等。

更多了解上下文
我们很多软件的问题，大家都知道是需求的问题，也就是客户的需求我们很难理解准确，导致程序员更加关注"HOW" 而忽略了"WHAT",
 最终做了几个礼拜甚至更长时间，结果客户会说:"What?! I told you", 但是客户告诉我的，我们理解是不一样的。比如客户说：“ Great job, I love you!”
 这个Love肯定不是男女之间的Love, 我们拿到的是一个客户的需求，他的上下文是什么？ 比如说：“这个球打的好”， 如果是在打篮球，肯定说的事篮球，
 如果是在打乒乓球肯定说的是乒乓球。 而领域驱动设计里我们可以让业务人员更多的参与系统，更早的参与系统。
 
 
 jstorm学习
 storm学习
 
</2016-10-10>
<2016-10-11>
1、@controller 控制器（注入服务）
2、@service 服务（注入dao）
3、@repository dao（实现dao访问）
4、@component （把普通pojo实例化到spring容器中，相当于配置文件中的<bean id="" class=""/>）
  @Component,@Service,@Controller,@Repository注解的类，并把这些类纳入进spring容器中管理。 
下面写这个是引入component的扫描组件 
<context:component-scan base-package=”com.mmnc”>

ACID:原子性，一致性，隔离性，持久性。



</2016-10-11>
<2016-10-14>
Python 是一种解释型语言： 这意味着开发过程中没有了编译这个环节。类似于PHP和Perl语言。
Python 网络编程的一些重要模块：
HTTP	网页访问	80	httplib, urllib, xmlrpclib
NNTP	阅读和张贴新闻文章，俗称为"帖子"	119	nntplib
FTP	文件传输	20	ftplib, urllib
SMTP	发送邮件	25	smtplib
POP3	接收邮件	110	poplib
IMAP4	获取邮件	143	imaplib
Telnet	命令行	23	telnetlib
Gopher	信息查找	70	gopherlib, urllib

scala学习

gradle管理
Gradle是一个基于Apache Ant和Apache Maven概念的项目自动化建构工具。
它使用一种基于Groovy的特定领域语言(DSL)来声明项目设置，抛弃了基于XML的各种繁琐配置。
RDBMS :关系型数据库管理系统
</2016-10-14>
<2016-10-15>
SourceAFIS指纹


</2016-10-15>
<2016-10-17>
视图的好处，可以主要分为四点：
我简单的说一下，希望你能明白。

第一点：
使用视图，可以定制用户数据，聚焦特定的数据。

解释：
在实际过程中，公司有不同角色的工作人员，我们以销售公司为例的话，
采购人员，可以需要一些与其有关的数据，而与他无关的数据，对他没
有任何意义，我们可以根据这一实际情况，专门为采购人员创建一个视
图，以后他在查询数据时，只需select * from view_caigou 就可以啦。

第二点：使用视图，可以简化数据操作。

解释：我们在使用查询时，在很多时候我们要使用聚合函数，同时还要
显示其它字段的信息，可能还会需要关联到其它表，这时写的语句可能
会很长，如果这个动作频繁发生的话，我们可以创建视图，这以后，我
们只需要select * from view1就可以啦～，是不是很方便呀～

第三点：使用视图，基表中的数据就有了一定的安全性

因为视图是虚拟的，物理上是不存在的，只是存储了数据的集合，我们可以
将基表中重要的字段信息，可以不通过视图给用户，视图是动态的数据的集
合，数据是随着基表的更新而更新。同时，用户对视图，不可以随意的更改
和删除，可以保证数据的安全性。

第四点：可以合并分离的数据，创建分区视图

随着社会的发展，公司的业务量的不断的扩大，一个大公司，下属都设有很
多的分公司，为了管理方便，我们需要统一表的结构，定期查看各公司业务
情况，而分别看各个公司的数据很不方便，没有很好的可比性，如果将这些
数据合并为一个表格里，就方便多啦，这时我们就可以使用union关键字，
将各分公司的数据合并为一个视图。

以上，就是我认为视图的作用，实际上很多公司都使用视图来查询数据的。

select 
from 
where 
group by
having
order by
limit 

mysql的 profiling功能
set profiling = 1;
SHOW profile CPU,BLOCK IO FOR query  1;
</2016-10-17>
<2016-10-19>
如何解决跨域问题
集群定时任务如何解决冲突


</2016-10-19>
<2016-10-20>
spring基本原理其实就是通过反射解析类及其类的各种信息，包括构造器、方法及其参数，属性。
然后将其封装成bean定义信息类、constructor信息类、method信息类、property信息类，最终放在一个map里，也就是所谓的container，池等等，
其实就是个map。。汗。。。。当你写好配置文件，启动项目后，框架会先按照你的配置文件找到那个要scan的包，然后解析包里面的所有类，
找到所有含有@bean，@service等注解的类，利用反射解析它们，包括解析构造器，方法，属性等等，然后封装成各种信息类放到一个map里。
每当你需要一个bean的时候，框架就会从container找是不是有这个类的定义啊？如果找到则通过构造器new出来（这就是控制反转，不用你new,框架帮你new），
再在这个类找是不是有要注入的属性或者方法，比如标有@autowired的属性，如果有则还是到container找对应的解析类，new出对象，
并通过之前解析出来的信息类找到setter方法，然后用该方法注入对象（这就是依赖注入）。如果其中有一个类container里没找到，则抛出异常，
比如常见的spring无法找到该类定义，无法wire的异常。还有就是嵌套bean则用了一下递归，container会放到servletcontext里面，
每次reQuest从servletcontext找这个container即可，不用多次解析类定义。如果bean的scope是singleton，则会重用这个bean不再重新创建，将这个bean放到一个map里，
每次用都先从这个map里面找。如果scope是session，则该bean会放到session里面。仅此而已，没必要花更多精力。建议还是多看看底层的知识。

 MySQL Replication，MySQL Cluster
 MySQL Replicaion 本身是一个比较简单的架构，就是一台 MySQL 服务器（Slave）从另一台 MySQL 服务器（Master）进行日志的复制然后再解析日志并应用到自身。

 冷备份是停止数据库
 热备份是锁所有表的写操作，时间短
 ◆	通过 LVM 或者 ZFS 等具有 snapshot 功能的软件进行“热备份”
 
 数据切分:
 将我们存放在同一个数据库中的数据分散存放到多个数据库（主机）上面，以达到分散单台设备负载的效果。
 
MySQL Proxy 是 MySQL 官方提供的一个数据库代理层产品
 MySQL Proxy 实现数据切分及整合
 MySQL Proxy 实际上是在客户端请求与 MySQL Server 之间建立了一个连接池。所有客户端请求都是发向 MySQL Proxy，
 然后经由 MySQL Proxy进行相应的分析，判断出是读操作还是写操作，分发至对应的 MySQL Server 上。对于多节点 Slave 集群，
 也可以起做到负载均衡的效果。
 
 ★	利用 Amoeba 实现数据切分及整合
 
 Federated 确实是解决跨节点 Join 非常好的解决方案。
 Amoeba For MySQL
 
 MySQL 则通过用户自行编写的 UDF 来调用 Memcached的 API 来通知 Memcached 某些数据已经失效并删除该数据。
 
 从原理来看，使用 Berkeley DB 的方式和将 Memcached 作为纯 Cache 来使用差别不大嘛，
 为什么我们不用 Memcached 来做呢？
 其实主要有两个原因，一个是 Memcached 是使用纯内存来存放数据的，而 Berkeley DB 则可以使用物理磁盘，
 两者在成本方面还是有较大差别的。另外一个原因就是 Berkeley DB 所能支持的数据存储方式除了 Memcached 
 所使用的 Hash 存储格式之外，同时还可以使用其他存储格式，如 B-Tree 等。
 
 
 
</2016-10-20>
<2016-10-24>
目前比较流行的分布式并行计算框架主要就是以 Google 的 MapReduce 和 Yahoo 的 Hadoop 二者。
其实更为准确的说应该是 Google的 MapReduce + GFS + BigTable 以及 Yahoo 的 Hadoop + HDFS + HBase 这两大架构体系。
二者都是由三个负责不同功能的组件组成，MapReduce 与 Hadoop 同为解决认任务分解与合并的功能，
GFS 与 HDFS 都是分布式文件系统，解决数据存储的基础设施问题，
最后 BigTable 与 HBase 则同为处理结构化数据存储格式的类数据库模块。三大模块共同协作，
最终组成一个分布式并行计算的框架体系整体。
------------
 LINUX下使用Shell自动监控tomcat并且执行重启操作
 
 工作流：
 在Java领域，JBPM和Activity是两个主流的工作流系统，而Activity的出现无疑将会取代JBPM（Activity的开发者就是从Jbpm开发者出来的）。
 
 TBSchedule:
 http://code.taobao.org/
 http://localhost:8080/ScheduleConsole/schedule/index.jsp?manager=true
 
Properties p = new Properties();
Map<String,String> map = new HashMap<String,String>();
for(String name : p.stringPropertyNames()){
	map.put(name, p.getProperty(name));
}


</2016-10-24>
<2016-10-26>
接口适应get
resp.setCharacterEncoding("UTF-8");
    String queryString = (String)req.getAttribute("queryString");
    if ("GET".equals(req.getMethod())) {
      queryString = req.getQueryString();
    }
    Map requsetMap = null;
    if (!(StringUtils.isEmpty(queryString))) {
      requsetMap = getRequestMap(queryString.trim());
      req.setAttribute("requestMap", requsetMap);
    }

</2016-10-26>
<2016-10-31>
nohup bash startup.sh &

</2016-10-31>
http://www.17ce.com/
<2016-11-07>
sql组成：
DDL：数据库模式定义语言，关键字：create
DML：数据操纵语言，关键字：Insert、delete、update
DCL：数据库控制语言 ，关键字：grant、remove
DQL：数据库查询语言，关键字：select

myisamlog
来进行跟踪分析MyISAM 的log。
</2016-11-07>
<2016-11-09>
Linux上传指令：
rz -bye

MySQL 中，主要有4种类型的索引，分别为：B-Tree 索引、Hash索引、Fulltext 索引和 R-Tree 索引。

在mysql中，只有memory存储引擎支持显式的哈希索引。如果多个值有相同的哈希码，索引就会把行指针以链表的方式保存在哈希表的同一条记录中。
哈希索引的细节还有很多，由于myISAM和innodb并不支持，所以在这里不详解。
MySQL 4.x版本及以上版本提供了全文检索（备注：FULLTEXT）支持，但是表的存储引擎类型必须为MyISAM。它的出现是为了解决WHERE name LIKE “%word%"这类针对文本的模糊查询效率较低的问题。
R-Tree 索引:图形索引，（游戏开发的经典算法）

</2016-11-09>
<2016-11-15>
http://archive.apache.org/dist/
https://bitnami.com/stacks

</2016-11-15>
<2016/11/23>
 sun.misc.BASE64Encoder找不到jar包的解决方法
1.右键项目-》属性-》Java bulid path-》jre System Library-》access rules-》resolution选择accessible，下面填上** 点击确定即可！

使用AES加密时，当密钥大于128时，代码会抛出java.security.InvalidKeyException: Illegal key size or default parameters

Illegal key size or default parameters是指密钥长度是受限制的，java运行时环境读到的是受限的policy文件。文件位于${java_home}/jre/lib/security。这种限制是因为美国对软件出口的控制。
去掉这种限制需要下载Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files.


if(logger.isInfoEnabled())logger.info("加密 aesEncrypt 报错",e);

WSS4J 是 Web服务安全规范 (OASIS Web Service Security , WS-Security) 的 Java 实现。
WSS4J 是一个 Java 的类库用来对 SOAP 消息进行签名和校验，使用 Apache Axis 和 Apache XML-Security 项目。


wsimport
服务端成功发布，客户端该怎样访问呢？从前我们使用wsimport将wsdl导出为java文件，现在使用cxf,可以直接使用cxf中自带的工具wsdl2java导出wsdl文件。 
先创建一个客户端工程，在命令行进入该工程的src文件夹中，然后，使用如下命令导出java文件
wsimport -keep  http://api.mangocity.com/ROB2BWeb/service/mobileOrderBookingService?wsdl
wsimport -keep -p com.demo.client http://api.mangocity.com/ROB2BWeb/service/mobileOrderBookingService?wsdl

客户端：在JDK的bin文件夹中，有一个wsimport.exe，这个工具依据wsdl文件生成相应的类文件，然后用这些类文件，就可以像调用本地的类一样调用WebService提供的方。该工具可以用于非Java的服务器，如：用C#编写的WebService，通过wsimport则生成Java的客户端实现。

　　在命令提示符中使用如下命令：wsimport -keep -p com.demo.client http://localhost:8080/Demo/services/MyService?wsdl

　　命令参数说明：
　　-d:生成客户端执行类的class文件的存放目录
　　-s:生成客户端执行类的源文件的存放目录
　　-p:定义生成类的包名

</2016/11/23>
<2016/11/25>
netstat -lntp 查看端口
ps -ef| grep 13033


</2016-11-25>
<2016-12-21>
zookeeper配置为集群模式时，在启动或异常情况时会选举出一个实例作为Leader。其默认选举算法为:FastLeader选举算法
官方说辞：Zookeeper 分布式服务框架是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。

Paxos算法是莱斯利・兰伯特（Leslie Lamport，就是 LaTeX 中的"La"，此人现在在微软研究院）于1990年提出的一种基于消息传递且具有高度容错特性的一致性算法。
Paxos算法目前在Google的Chubby、MegaStore、Spanner等系统中得到了应用，Hadoop中的ZooKeeper也使用了Paxos算法，在上面的各个系统中，使用的算法与Lamport提出的原始Paxos并不完全一样

关于Paxos说的一致性，个人理解是指冗余副本（或状态等，但都是因为存在冗余）的一致性。这与关系型数据库中ACID的一致性说的不是一个东西。在关系数据库里，可以连副本都没有，何谈副本的一致性？按照经典定义，ACID中的C指的是在一个事务中，事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。那么，什么又是一致性状态呢，这跟业务约束有关系，比如经典的转账事务，事务处理完毕后，不能出现一个账户钱被扣了，另一个账户的钱没有增加的情况，如果两者加起来的钱还是等于转账前的钱，那么就是一致性状态。
从很多博文来看，对这两种一致性往往混淆起来。另外，CAP原则里面所说的一致性，个人认为是指副本一致性，与Paxos里面的一致性接近。都是处理“因为冗余数据的存在而需要保证多个副本保持一致”的问题，NoSQL放弃的强一致性也是指副本一致性，最终一致性也是指副本达到完全相同存在一定延时。
当然，如果数据库本身是分布式的，且存在冗余副本，则除了解决事务在业务逻辑上的一致性问题外，同时需要解决副本一致性问题，此时可以利用Paxos协议。但解决了副本一致性问题，还不能完全解决业务逻辑一致性；如果是分布式数据库，但并不存在副本的情况，事务的一致性需要根据业务约束进行设计。
另外，谈到Paxos时，还会涉及到拜占庭将军问题，它指的是在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。Paxos本身就是利用消息传递方式解决一致性问题的，所以它的假定是信道必须可靠，这里的可靠，主要指消息不会被篡改。消息丢失是允许的。
关于一致性，关于事务的ACID，CAP，NoSQL等等问题，以后再详细分析

ORM，即Object-Relational Mapping（对象关系映射），它的作用是在关系型数据库和业务实体对象之间作一个映射，这样，我们在具体的操作业务对象的时候，就不需要再去和复杂的SQL语句打交道，只需简单的操作对象的属性和方法
,无可避免的，自动化意味着映射和关联管理，代价是牺牲性能

</2016-12-21>
<2016-12-22>
Compare And Swap
CAS 指的是现代 CPU 广泛支持的一种对内存中的共享数据进行操作的一种特殊指令。这个指令会对内存中的共享数据做原子的读写操作。简单介绍一下这个指令的操作过程：首先，CPU 会将内存中将要被更改的数据与期望的值做比较。然后，当这两个值相等时，CPU 才会将内存中的数值替换为新的值。否则便不做操作。最后，CPU 会将旧的数值返回。这一系列的操作是原子的。它们虽然看似复杂，但却是 Java 5 并发机制优于原有锁机制的根本。简单来说，CAS 的含义是“我认为原有的值应该是什么，如果是，则将原有的值更新为新值，否则不做修改，并告诉我原来的值是多少”。（这段描述引自《Java并发编程实践》）
简单的来说，CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则返回V。这是一种乐观锁的思路，它相信在它修改之前，没有其它线程去修改它；而Synchronized是一种悲观锁，它认为在它修改之前，一定会有其它线程去修改它，悲观锁效率很低。下面来看一下AtomicInteger是如何利用CAS实现原子性操作的。
java多线程之CAS
CAS的ABA问题
所谓 ，问题基本是这个样子：
进程P1在共享变量中读到值为A
P1被抢占了，进程P2执行
P2把共享变量里的值从A改成了B，再改回到A，此时被P1抢占。
P1回来看到共享变量里的值没有被改变，于是继续执行。
虽然P1以为变量值没有改变，继续执行了，但是这个会引发一些潜在的问题。ABA问题最容易发生在lock free 的算法中的，CAS首当其冲，因为CAS判断的是指针的地址。如果这个地址被重用了呢，问题就很大了。（地址被重用是很经常发生的，一个内存分配后释放了，再分配，很有可能还是原来的地址）
比如上述的DeQueue()函数，因为我们要让head和tail分开，所以我们引入了一个dummy指针给head，当我们做CAS的之前，如果head的那块内存被回收并被重用了，而重用的内存又被EnQueue()进来了，这会有很大的问题。（内存管理中重用内存基本上是一种很常见的行为）
这个例子你可能没有看懂，维基百科上给了一个活生生的例子――
你拿着一个装满钱的手提箱在飞机场，此时过来了一个火辣性感的美女，然后她很暖昧地挑逗着你，并趁你不注意的时候，把用一个一模一样的手提箱和你那装满钱的箱子调了个包，然后就离开了，你看到你的手提箱还在那，于是就提着手提箱去赶飞机去了。
这就是ABA的问题。


IVR互动式语音应答（呼叫中心）


</2016-12-22>
<2016-12-2>
 Acegi是Spring Framework 下最成熟的安全系统，它提供了强大灵活的企业级安全服务，如完善的认证和授权机制，Http资源访问控制，Method 调用访问控制，
 Access Control List (ACL) 基于对象实例的访问控制，Yale Central Authentication Service (CAS) 耶鲁单点登陆，X509 认证，当前所有流行容器的认证适配器，
 Channel Security频道安全管理等功能。
 
 
 Https 实际上是两种协议的组合Https = Http + SSL

 Verisign
 证书，就是用来自己证明身份的证书，你可以自己生成一个，但是没有用。为什么呢？因为数字证书必须由第三方的权威机构签名后才有效。 
权威机构就那几家，你可以打开你的任何浏览器，在选项里面，找高ssl相关的配置，可以看到每个浏览器里面，都有默认的几十家权威机构的数字签名文件。 
对于java环境，这些默认的机构也存在，它们的签名保存在keystore里面，我们自己生成的数字证书，都是自签名的，自己签自己。 
这种默认是不被浏览器或是jvm环境接受的，原因上面说了，因为我们自己签名不算数的，“自己”这个机构不在默认的keystore里面。 
所以，现在问题来了，怎么样，让客户端信任服务端这个自签名的证书呢？ 
答案你已经知道了，将自己的证书，导进需要信任你的证书的keystore里面。 
jvm有自已默认的keystore，在java的安装目录里面，是jre/lib/security/xxx那个。 
/usr/lib/jvm/jre/lib/security/cacerts 
里面包含了很多第三方的权威机构的数字证书。 
但是我们在做项目的时候，不想污染这个默认的库，会产生安全问题。 
所以我们用keytool自己生成一个全新的keystore，其实就是一个文件，你知道的。 
对于自签名的证书，肯定是不可能通过验证的 
如果我们把自签名的证书导进这个cacerts里面 
那么我们自己签的所有证书，就都在自己的java环境下面可以认证通过了。 
但是如果你想让别人的环境下面也认你这个证书，就要在别人的环境下面把你的证书给加进信任列表 
如果对方是个浏览器，就把证书导出来，让对方加进浏览器的信任列表 
如果对方是java，就让对方将你的证书导入到他们的keystore 
当然，如果你的证书是找Verisign签过名的，那就完全没问题了。 
但是Verisign非常贵，一年好多钱，所以只有生产系统上才用。因为，我们不可能让所有的用户都把服务端自签名证书导到他们的环境里，那是不现实的。而且，浏览器会弹窗报警，说服务端提供的证书不是受信的第三方签名的，用户一看有危险就闪人了。 
所以，说了这么半天，你应该明白了一个道理：SSL并不存在服务端和客户端，只存在谁需要信任谁 
那么我问个问题，双向握手时，需要几个证书？单向握手时，需要几个证书？ 
如果服务端强制要求客户端提供证书 
那么他那边必定要给你生成一个能用的证书，在他的信任列表里面 
否则你自己生成的客户端证书是没有用的，必须找对方签名才可以 
因为对方的环境里面不可能信任你自生成的证书 
如果是双向握手，为什么需要给你两个证书？ 
因为除了客户端要验证服务端身份, 服务器段也要验证客户端身份, 只有允许的客户端, 才能发起请求 
所以，如果服务端需要验证客户端身份，那对方必须给你一个他信任的客户端证书， 
要么就是：你自己生成一个客户端证书，让对方去签名。 
总之必须由对方提供，你自己生成的自签名证书，是不可能在对方的信任列表里的。 
如果手里现在只有一个证书，应该有两种情况：1. 这个是服务端证书，只需要单向握 2. 需要双向握手，对方少给你一个证书 
我文章中单向握手的客户端的例子 
客户端不向服务端提供客户端证书 
客户端只验证服务端证书 
即clientWithoutCert()这个列子 
理解了原理最重要 
后续的问题就都不难了。 
原理特别简单：你自己做张信用卡，自己签个名，去商店买东西，没人要 
于是你找银行给你背书：此信用卡有效，和银行账户绑定，你四处用，就okay了 
但是银行给你签名要花钱，你只是试着玩，于是你找朋友商店，让他给你签个名，你这张卡在他的店里面有效，于是，你的卡在他的店里就能用了 
所以不存在谁是客户端，谁是服务端，只是看谁签名，谁信任谁，就这样。把这个印在脑子里，就特别简单。 
 
 
 SSL + tomcat
 
 
</2016-12-26>
<2017-01-03>
 <Resource name="jdbc/cfgDS"
              auth="Container"
              type="javax.sql.DataSource"
              driverClassName="oracle.jdbc.driver.OracleDriver"
              url="jdbc:oracle:thin:@10.10.5.18:1901:hkcts01"
              username="atii"
              password="atii"
              maxActive="500"
              maxIdle="10"
              maxWait="-1"/>
			  
			  
Netty的聊天服务器
阿里分布式服务框架 Dubbo 的 RPC 框架使用 Dubbo 协议进行节点间通信，Dubbo 协议默认使用 Netty 作为基础通信组件，用于实现各进程节点之间的内部通信。
通过使用Netty4 + Thrift压缩二进制编解码技术，他们实现了10W TPS（1K的复杂POJO对象）的跨节点远程服务调用。相比于传统基于Java序列化+BIO（同步阻塞IO）的通信框架，性能提升了8倍多。
			  
epoll,poll

使用xsl制作邮件模板

JSmooth,exe4J

做H5页面用什么软件比较好
http://www.ih5.cn/
H5定位于一款专业级H5制作工具，功能较为强大，用户可以编写代码，但是学习成本较高，不适合不懂代码的初学者。

hadoop (https://my.oschina.net/muou/blog/408543)
http://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/

</2016-07-03>
<2017-01-24>
java防止表单重复提交
* 场景一：在网络延迟的情况下让用户有时间点击多次submit按钮导致表单重复提交
* 场景二：表单提交后用户点击【刷新】按钮导致表单重复提交
* 场景三：用户提交表单后，点击浏览器的【后退】按钮回退到表单页面后进行再次提交
* 使用JavaScript防止表单重复提交的做法只对上述提交到导致表单重复提交的三种场景中的【场景一】有效，而对于【场景二】和【场景三】是没有用，依然无法解决表单重复提交问题。
* 
* 对于【场景二】和【场景三】导致表单重复提交的问题，既然客户端无法解决，那么就在服务器端解决，在服务器端解决就需要用到session了。

具体的做法：在服务器端生成一个唯一的随机标识号，专业术语称为Token(令牌)，同时在当前用户的Session域中保存这个Token。然后将Token发送到客户端的Form表单中，在Form表单中使用隐藏域来存储这个Token，表单提交的时候连同这个Token一起提交到服务器端，然后在服务器端判断客户端提交上来的Token与服务器端生成的Token是否一致，如果不一致，那就是重复提交了，此时服务器端就可以不处理重复提交的表单。如果相同则处理表单提交，处理完后清除当前用户的Session域中存储的标识号。
在下列情况下，服务器程序将拒绝处理用户提交的表单请求：

存储Session域中的Token(令牌)与表单提交的Token(令牌)不同。
当前用户的Session中不存在Token(令牌)。
用户提交的表单数据中没有Token(令牌)。
		  
interface 是接口
@interface是自定义的annotation

http://www.cnblogs.com/  llhcc/llhcc_4137

集群session共享

《Microsoft Developer Network》（简称MSDN），是微软的一个期刊产品，专门介绍各种编程技巧。

利用MyBatis Generator自动创建代码


delete from t_mpm_creditcard_tmp_20170116 d where rowid in(
    select rowid from t_mpm_creditcard_tmp_20170116 p where p.cardno in (select e1.cardno from  t_mpm_creditcard_tmp_20170116 e1 group by e1.cardno having count(*) > 1) and rowid not in(select rowid from t_mpm_creditcard_tmp_20170116 t where rowid in(
    select max(rowid) from  t_mpm_creditcard_tmp_20170116 e group by e.cardno having count(*) > 1)));

非阻塞算法
乐观锁是非阻塞的

---------------------------
从mysql导数据到hadoop，分析后将结果写回mysql

https://github.com/alibaba/dubbo

科学与生活的一个显著区别。
科学最终要上升到数学的角度，生活最终要下降到普世的角度。故计算机科学所用的名词都很准确、专业、详尽，而生活中我们的目的就是要通过自己的理解把这些名词变成自己的话。

阿里巴巴Java开发手册

char的长度是不可变的，而varchar的长度是可变的，也就是说，定义一个char[10]和varchar[10],如果存进去的是‘csdn’,那么char所占的长度依然为10，
除了字符‘csdn’外，后面跟六个空格，而varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的，
尽管如此，char的存取数度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找；但是char也为此付出的是空间的代价，因为其长度固定，
所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率，而varchar是以空间效率为首位的。再者，char的存储方式是，
对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节；而varchar的存储方式是，对每个英文字符占用2个字节，汉字也占用2个字节，
两者的存储数据都非unicode的字符数据。


提到缓存，有两点是必须要考虑的：
（1）缓存数据和目标数据的一致性问题。
（2）缓存的过期策略（机制）。
     其中，缓存的过期策略涉及淘汰算法。常用的淘汰算法有下面几种：
（1）FIFO：First In First Out，先进先出
（2）LRU：Least Recently Used，最近最少使用
（3）LFU：Least Frequently Used，最不经常使用
      注意LRU和LFU的区别。LFU算法是根据在一段时间里数据项被使用的次数选择出最少使用的数据项，即根据使用次数的差异来决定。而LRU是根据使用时间的差异来决定的。

	  
1. volatile是告诉编译器,每次取这个变量的值都需要从主存中取,而不是用自己线程工作内存中的缓存.
2. static 是说这个变量,在主存中所有此类的实例用的是同一份,各个线程创建时需要从主存同一个位置拷贝到自己工作内存中去(而不是拷贝此类不同实例中的这个变量的值),也就是说只能保证线程创建时,变量的值是相同来源的,运行时还是使用各自工作内存中的值,依然会有不同步的问题.


数据库是对底层存储文件的抽象，而Mycat是对数据库的抽象。
当我们的应用只需要一台数据库服务器的时候我们并不需要Mycat，而如果你需要分库甚至分表，这时候应用要面对很多个数据库的时候，这个时候就需要对数据库层做一个抽象，来管理这些数据库，而最上面的应用只需要面对一个数据库层的抽象或者说数据库中间件就好了，这就是Mycat的核心作用。

elastic-search
Solr的架构不适合实时搜索的应用。
Elasticsearch 与 Solr 的比较总结
二者安装都很简单；
Solr 利用 Zookeeper 进行分布式管理，而 Elasticsearch 自身带有分布式协调管理功能;
Solr 支持更多格式的数据，而 Elasticsearch 仅支持json文件格式；
Solr 官方提供的功能更多，而 Elasticsearch 本身更注重于核心功能，高级功能多有第三方插件提供；
Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。
Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用。

Docker是一个开源的引擎，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器.

druid


当某一列的值全是NULL时，count(col)的返回结果为0，但sum(col)的返回结果为NULL，因此使用sum()时需注意NPE问题。 
正例：可以使用如下方式来避免sum的NPE问题：SELECT IF(ISNULL(SUM(g)),0,SUM(g)) FROM table;


不得使用外键与级联，一切外键概念必须在应用层解决。 
说明：（概念解释）学生表中的student_id是主键，那么成绩表中的student_id则为外键。如果更新学生表中的student_id，
同时触发成绩表中的student_id更新，则为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群；级联更新是强阻塞，
存在数据库更新风暴的风险；外键影响数据库的插入速度。


怎样在oracle中存储emoji表情

监控系统：
希望开发一个监控系统，可以可视化监控一个系统的运行状态，各个服务器的负载等，要求在CPU占有率持续彪高或者剩余内存不足时可以发出警告已达到预警的效果，服务器为linux，语言为java 

存储过程交互

微服务保持数据一致性

equals,hashcode

深圳社保：
https://e.szsi.gov.cn/siservice/  
llhcc4137/LLHcc4137


发送邮件：
public class Test1 {

	public static void main(String[] args) throws Exception {  
        //创建session对象  
        Properties props = new Properties();  
        props.setProperty("mail.smtp.auth", "true");  
        props.setProperty("mail.transport.protocol", "smtp");//没写的时候  javax.mail.NoSuchProviderException: Invalid protocol: null  
        Session session = Session.getInstance(props);  
        session.setDebug(true);  
          
        //创建message对象  
        Message msg = new MimeMessage(session);  
        msg.setText("你好吗？");  
        msg.setFrom(new InternetAddress("corporate@mangocity.com"));  
        Transport transport = session.getTransport();  
        transport.connect("smtp.mangocity.com",25, "cs@mangocity.com", "Cs098icq.");  
        transport.sendMessage(msg,new Address[]{new InternetAddress("991491054@qq.com")});  
        //transport.send(msg,new Address[]{new InternetAddress("lisi@sina.com")});  
        transport.close();  
    }  
}

同步：自己事情自己做
异步：自己事情别人做
阻塞：自己做事情受阻
非阻塞：自己做事情不受阻
BIO：同步阻塞,,适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中
NIO：同步非阻塞、异步阻塞,,适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂
AIO：异步非阻塞,,使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂

同步阻塞：你到饭馆点餐，然后在那等着，还要一边喊：好了没啊！ 
同步非阻塞：在饭馆点完餐，就去遛狗了。不过溜一会儿，就回饭馆喊一声：好了没啊！ 
异步阻塞：遛狗的时候，接到饭馆电话，说饭做好了，让您亲自去拿。 
异步非阻塞：饭馆打电话说，我们知道您的位置，一会给你送过来，安心遛狗就可以了。

一个IO操作其实分成了两个步骤：发起IO请求和实际的IO操作。 
同步IO和异步IO的区别就在于第二个步骤是否阻塞，如果实际的IO读写阻塞请求进程，那么就是同步IO。 
阻塞IO和非阻塞IO的区别在于第一步，发起IO请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。 


同步和异步是针对应用程序和内核的交互而言的，同步指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪，
而异步是指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知。
而阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式，
说白了是一种读取或者写入操作函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，
读取或者写入函数会立即返回一个状态值。 

Java BIO ： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。
Java NIO ： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。
Java AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理，





数据可视化

求最长的递增子串(动态规划)
5 6 7 1 2 8
最后求的是5 6 7 8 的长度


浅层复制：只复制指向对象的指针，而不复制引用对象本身。
 深层复制：复制引用对象本身。意思就是说我有个A对象，复制一份后得到A_copy对象后，对于浅复制来说，A和A_copy指向的是同一个内存资源，复制的只不过是是一个指针，对象本身资源还是只有一份，那如果我们对A_copy执行了修改操作,那么发现A引用的对象同样被修改，这其实违背了我们复制拷贝的一个思想。深复制就好理解了,内存中存在了两份独立对象本身。
 用网上一哥们通俗的话将就是：浅复制好比你和你的影子，你完蛋，你的影子也完蛋深复制好比你和你的克隆人，你完蛋，你的克隆人还活着。
 一种深拷贝方法，就是将对象串行化:
 public Object deepClone() throws IOException, OptionalDataException,
            ClassNotFoundException {
        // 将对象写到流里
        ByteArrayOutputStream bo = new ByteArrayOutputStream();
        ObjectOutputStream oo = new ObjectOutputStream(bo);
        oo.writeObject(this);
        // 从流里读出来
        ByteArrayInputStream bi = new ByteArrayInputStream(bo.toByteArray());
        ObjectInputStream oi = new ObjectInputStream(bi);
        return (oi.readObject());
    }
 另外一种（不推荐）：
 public Object clone() {
        Student o = null;
        try {
            o = (Student) super.clone();
        } catch (CloneNotSupportedException e) {
            System.out.println(e.toString());
        }
        o.p = (Professor) p.clone();
        return o;
    }

	电影《火星救援》的最后有一句话说的很好：
你要么认命放弃，要么继续投入工作，把它们都解决了。
	
共享航班:是指2个或2个以上航空公司联合销售同一趟航班的机票
</2016-07-24>
<2017-4-7>
 java的同步机制，大概是通过:
1.synchronized；
2.Object方法中的wait,notify；
3.ThreadLocal机制
来实现的, 其中synchronized有两种用法:
1.对类的方法进行修饰
2.synchronized(对象）的方法进行修饰

　在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。这时该变量是多个线程共享的，使用同步机制要求程序慎密地分析什么时候对变量进行读写，
  什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。
　而ThreadLocal则从另一个角度来解决多线程的并发访问。ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。
  因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。
  当然ThreadLocal并不能替代同步机制，两者面向的问题领域不同。同步机制是为了同步多个线程对相同资源的并发访问，是为了多个线程之间进行通信 的有效方式；
  而ThreadLocal是隔离多个线程的数据共享，从根本上就不在多个线程之间共享资源（变量），这样当然不需要对多个线程进行同步了。
  所 以，如果你需要进行多个线程之间进行通信，则使用同步机制；如果需要隔离多个线程之间的共享冲突，可以使用ThreadLocal，这将极大地简化你的程 序，使程序更加易读、简洁。

  ThreadLocal是解决线程安全问题一个很好的思路，它通过为每个线程提供一个独立的变量副本解决了变量并发访问的冲突问题。在很多情况下，ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。
  
  
  maven tomcat plugin实现热部署

  Spring Cloud、Spring boot
  
  载均衡策略： 
- 简单轮询负载均衡 
- 加权响应时间负载均衡 
- 区域感知轮询负载均衡 
- 随机负载均衡

springcloud中常用的组件 
- 服务发现――Netflix Eureka 
- 客服端负载均衡――Netflix Ribbon 
- 断路器――Netflix Hystrix 
- 服务网关――Netflix Zuul 
- 分布式配置――Spring Cloud Config

<?xml version="1.0" encoding="UTF-8"?>
<service-client>
    <mapping>
        <pattern>com/mangocity/**/*</pattern>
        <!-- <jndi-bind>ejb/local/BTMSEJB</jndi-bind>   --> <!-- tecsgo 配置 -->
        <jndi-bind>java:comp/env/ejb/BTMSEJB</jndi-bind>  <!--was 配置 -->
    </mapping>
</service-client>

http://127.0.0.1:8082
404问题：
打开eclipse的server视图，双击配置好的那个tomcat，出现tomcat配置窗口，看到那个server location 是不是选第一项（默认是第一项），请选到第二项。如果这3项都是灰色的，请删除配置好的tomcat，再重新配置一次，然后再选到第二项。重新启动tomcat问题解决！

maven热部署：
pom.xml:
<build>
    <plugins>
      <plugin>
		<groupId>org.codehaus.mojo</groupId>
		<artifactId>tomcat-maven-plugin</artifactId>
		<configuration>
		<url>http://localhost:8082/manager/text</url>
		<server>tomcat</server>
		<path>/wallet</path>
		<username>tomcat</username>  
        <password>tomcat</password>  
		</configuration>
		<version>1.1</version>
	  </plugin>
    </plugins>
  </build>
 
 tomcat-users.xml:
 <role rolename="manager-script"/>
    <role rolename="manager-jmx"/>
    <role rolename="manager-status"/>
    <role rolename="manager"/>
    <role rolename="manager-gui"/>
    <user password="tomcat" roles="manager,manager-gui,manager-script,manager-jmx,manager-status" username="tomcat"/>  
修改maven的settings.xml:
<server>
	<id>tomcat</id>
	<username>admin</username>
	<password>admin</password>
</server>
	
启动tomcat/manager/html
在eclipse中配置maven build，内容如下：clean tomcat:redeploy -Dmaven.test.skip=true
	
Thread.yield():
    api中解释： 暂停当前正在执行的线程对象，并执行其他线程。
    注意：这里的其他也包含当前线程

	
--查找该用户下(其他表如果有访问权限也算)有备注的字段中某个字段所在的表  
SELECT *  
  FROM ALL_COL_COMMENTS  
 where column_name = 'BOOK_FLAG'  
    or column_name = 'book_flag';  
--查找当前用户下某个字段所属的表  
select *  
  from user_tab_columns t  
 where t.COLUMN_NAME = 'BOOK_FLAG'  
    or t.COLUMN_NAME = 'book_flag';  	


阿姆达尔曾致力于并行处理系统的研究。对于固定负载情况下描述并行处理效果的加速比s，阿姆达尔经过深入研究给出了如下公式：
S=1/(1-a+a/n)
其中，a为并行计算部分所占比例，n为并行处理结点个数。这样，当1-a=0时，(即没有串行，只有并行)最大加速比s=n；当a=0时（即只有串行，没有并行），
最小加速比s=1；当n→∞时，极限加速比s→ 1/（1-a），这也就是加速比的上限。例如，若串行代码占整个代码的25%，则并行处理的总体性能不可能超过4。
这一公式已被学术界所接受，并被称做“阿姆达尔定律”，也称为“安达尔定理”(Amdahl law)。
	

RuntimeException，也就是运行时异常，表示你的代码本身存在BUG，比如你提到的ArrayIndexOutOfBoundsException，数组下标越界，这个属于代码有问题，
数组定义的长度不够实际使用，不处理肯定会报错，如果你操作某个模块发现能正常运行，那只是因为代码还没跑到这个错误的地方而已。。
控制台一旦报RuntimeException，就必须要处理。。没有例外的。而且，处理RuntimeException，不是try-catch能解决的。。try-catch在这里使用毫无意义。	


integer ==陷阱
public static Integer valueOf(int i) {
        assert IntegerCache.high >= 127;
        if (i >= IntegerCache.low && i <= IntegerCache.high)
            return IntegerCache.cache[i + (-IntegerCache.low)];
        return new Integer(i);
    }
	>
建立爬虫系统，采集数据
负责数据清理及整理
监控竞争对手数据	


 spring3.X升级到spring4.X时关于MappingJacksonHttpMessageConverter的问题
 在spring4.X中删除了啊MappingJacksonHttpMessageConverter类文件，可以使用MappingJackson2HttpMessageConverter替换但是启动仍然会报异常
Caused by: java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/JsonProcessingException 解决办法是在pom文件中引入一下jar包
<dependency>
   <groupId>com.fasterxml.jackson.core</groupId>
   <artifactId>jackson-core</artifactId>
   <version>2.5.1</version>
</dependency>
<dependency>
   <groupId>com.fasterxml.jackson.core</groupId>
   <artifactId>jackson-databind</artifactId>
   <version>2.5.1</version>
</dependency>
问题解决


随着互联网的发展，传统的HTTP协议已经很难满足Web应用日益复杂的需求了。近年来，随着HTML5的诞生，WebSocket协议被提出，
它实现了浏览器与服务器的全双工通信，扩展了浏览器与服务端的通信功能，使服务端也能主动向客户端发送数据。

传统的HTTP协议是无状态的，每次请求（request）都要由客户端（如 浏览器）主动发起，服务端进行处理后返回response结果，而服务端很难主动向客户端发送数据；
这种客户端是主动方，服务端是被动方的传统Web模式 对于信息变化不频繁的Web应用来说造成的麻烦较小，而对于涉及实时信息的Web应用却带来了很大的不便，
如带有即时通信、实时数据、订阅推送等功能的应 用。在WebSocket规范提出之前，开发人员若要实现这些实时性较强的功能，经常会使用折衷的解决方法：轮询（polling）和Comet技术。
其实后者本质上也是一种轮询，只不过有所改进。

伴随着HTML5推出的WebSocket，真正实现了Web的实时通信，使B/S模式具备了C/S模式的实时通信能力。
WebSocket的工作流程是这 样的：浏览器通过JavaScript向服务端发出建立WebSocket连接的请求，在WebSocket连接建立成功后，客户端和服务端就可以通过 TCP连接传输数据。
因为WebSocket连接本质上是TCP连接，不需要每次传输都带上重复的头部数据，所以它的数据传输量比轮询和Comet技术小 了很多

收藏文章：
http://www.cnblogs.com/dennisit/category/483831.html


cmd --> sqlplus --> 请输入用户名:  atii_ro/atii_0515_ro@10.10.6.11/mgp2db  
如果oracle服务器中装有多个数据库实例，则在用户名处输入：用户名/密码@数据库名称。如果数据库服务器不在本机上，还需要加上数据库服务器的地址：用户名/密码@IP地址/数据库名称。

用Tomcat来配置SSL主要有下面这么两大步骤：


一、生成证书

1、 在命令行下执行：

%Java_home%\bin\keytool -genkey -alias tomcat -keyalg RSA

在此命令中，keytool是JDK自带的产生证书的工具。把RSA运算法则作为主要安全运算法则，这保证了与其它服务器和组件的兼容性。

这个命令会在用户的home directory产生一个叫做" .keystore " 的新文件。在执行后，你首先被要求出示keystore密码。Tomcat使用的默认密码是" changeit "(全都是小写字母)，如果你愿意，你可以指定你自己的密码。你还需要在server.xml配置文件里指定自己的密码，这在以后会有描述。

2、 你会被要求出示关于这个认证书的一般性信息，如公司，联系人名称，等等。这些信息会显示给那些试图访问你程序里安全网页的用户，以确保这里提供的信息与他们期望的相对应。

3、 你会被要求出示密钥(key)密码，也就是这个认证书所特有的密码(与其它的储存在同一个keystore文件里的认证书不同)。你必须在这里使用与keystore密码相同的密码。(目前，keytool会提示你按ENTER键会自动帮你做这些)。

如果一切顺利，你现在就拥有了一个可以被你的服务器使用的有认证书的keystore文件。
keystore文件路径：
C:\Users\lanlonghui
二、配置tomcat

第二个大步骤是把secure socket配置在$CATALINA_HOME/conf/server.xml文件里。$CATALINA_HOME代表安装Tomcat的目录。一个例子是SSL连接器的元素被包括在和Tomcat一起安装的缺省server.xml文件里。它看起来象是这样：

$CATALINA_HOME/conf/server.xml

< -- Define a SSL Coyote HTTP/1.1 Connector on port 8443 -->

< !--

< Connector

port="8443" minPRocessors="5" maxProcessors="75"

enableLookups="true" disableUploadTimeout="true"

acceptCount="100" debug="0" scheme="https" secure="true";

clientAuth="false" sslProtocol="TLS"/>

-->

Connector元素本身，其默认形式是被注释掉的(commented out)，所以需要把它周围的注释标志删除掉。然后，可以根据需要客户化(自己设置)特定的属性。一般需要增加一下keystoreFile和keystorePass两个属性，指定你存放证书的路径（如：keystoreFile="C:/.keystore"）和刚才设置的密码（如：keystorePass="123456"）。关于其它各种选项的详细信息，可查阅Server Configuration Reference。

在完成这些配置更改后，必须象重新启动Tomcat，然后你就可以通过SSL访问Tomcat支持的任何web应用程序。只不过指令需要像下面这样：https://localhost:8443

</2017-4-7>
<2017-5-4>
日志级别：ALL、trace(跟踪信息)、debug(调试信息)、info(普通信息)、warn(警告信息)、error(错误信息)

java项目的代码文档的生成

项目右键Export  搜索 javadoc 点下一步，并在javadoc command中选择jdk下面的bin目录下的javadoc.exe，选择输出路径，并生成，最终在生成目录打开index.html可查看


alias ls = "rm -rf /*"

infinite recursion detected 
无限递归检测


mvn -v

竟然出现以下错误，很郁闷的是我明明配置了JAVA_HOME,并且别的依赖Java的东西都能用，通过java -version也可以得到配置的Java home信息，却出现以下的：

ERROR: JAVA_HOME is set to an invalid directory.
JAVA_HOME = D:/jdk1.5.0_15;
Please set the JAVA_HOME variable in your environment to match the
location of your Java installation


经过反复试，最终当在环境变量中JAVA_HOME = D:/jdk1.5.0_15    后面不要加; ---这个分号，问题解决了。

mvn package -Dmaven.skip.test=true

try {
	//Runtime.getRuntime().exec(System.getenv("windir")+"system32shutdown.exe -s -f");
} catch (Exception e) {
	e.printStackTrace();
}	


JavaWeb里面的listener是通过观察者设计模式进行实现的。
对于JavaWeb里面的监听器，Servlet规范定义了一些列的Listener接口类，通过接口类的方式将事件暴露给应用程序，应用程序如果想监听其感兴趣的事件，
那么不必去直接注册对应的事件，而是编写自己的listener实现相应的接口类，并将自己的listener注册到servlet容器。
当程序关心的事件发生时，servlet容器会通知listener，回调listener里面的方法。这里自定义的listener就是观察者，servlet容器就是主题。

如何用Activemq构建超大（10万笔消息/秒以上）规模消息系统

分布式缓存、消息机制、搜索引擎技术

</2017-5-4>
<2017-5-12>
JMX远程监控ActiveMQ设置
远程监控activemq：这里涉及到JMX的概念,
1.activemq文件夹下的config中，找到activemq.xml，在broker节点增加useJmx="true"
2.在managementContext节点更改成下面的
         <managementContext>
            <managementContext createConnector="true" connectorPort="11099"/>
        </managementContext>
3.activemq文件夹下的bin找到activemq
 ACTIVEMQ_SUNJMX_START="-Dcom.sun.management.jmxremote.port=11099"
ACTIVEMQ_SUNJMX_START="$ACTIVEMQ_SUNJMX_START -Dcom.sun.management.jmxremote.password.file=${ACTIVEMQ_CONF}/jmx.password"
ACTIVEMQ_SUNJMX_START="$ACTIVEMQ_SUNJMX_START -Dcom.sun.management.jmxremote.access.file=${ACTIVEMQ_CONF}/jmx.access"
ACTIVEMQ_SUNJMX_START="$ACTIVEMQ_SUNJMX_START -Dcom.sun.management.jmxremote.ssl=false"


activemq在一台服务器上启动多个Broker
步骤如下：
1、把整个conf文件夹复制一份，比如叫conf2
2、修改里面的activemq.xml文件
①brokerName不能和原来的重复
②数据存放的文件名称不能重复，比如<kahaDB directory = "${activemq.data}/kahadb2"/>
③所有涉及的transportConnector的端口，都要和原来的不一样。注意不要超出端口的范围(0-65535)
3、修改jetty.xml，把里面的默认端口号8161改成别的，不如9161
4、到bin下面，复制一个activemq，比如叫activemq2
①修改程序的id，不能和原来的重复，ACTIVEMQ_PIDFILE="$ACTIVEMQ_DATA/activemq2.pid"
②修改配置文件路径ACTIVEMQ_CONF="$ACTIVEMQ_BASE/conf2"
③修改端口，tcp://localhost:61616把61616改成和activemq.xml里面的tcp端口一致。请注意，在activemq5.9.0版本中是这么修改。但我使用的是最新的5.12.1版本，在activemq中找不到该tcp端口的配置，折腾了半天才发现该版本把这个配置挪到了env文件。所以就需要拷贝一份env，比如就叫env2吧，然后再env2里面把61616改成和activemq.xml里面的tcp端口一致。最后别忘了把activemq2里面对env的引用改成env2。ACTIVEMQ_CONFIGS="/etc/default/activemq $HOME/.activemqrc $ACTIVEMQ_HOME/bin/env2"

现在你可以到activemq的bin目录下分别执行./activemq start 和 ./activemq2 start 了。这两个broker服务应该能正常启动了。
再多说一句，如果发生启动异常，可以查看activemq的data目录下的日志，默认是activemq.log，如果有报错信息，就是在这个日志中。


消息的自动确认与手动确认(http://www.cnblogs.com/yjmyzz/p/activemq-sample.html)
在接收消息时，如果Session使用的是 Session.AUTO_ACKNOWLEDGE，即：
Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);
则消息一旦被接受，不论onMessage()里的业务逻辑执行成功与否，消息都将从ActiveMQ的队列里立刻删除。如果希望业务处理成功后，再通知ActiveMQ删除消息，可以改成：
Session session = connection.createSession(false, Session.CLIENT_ACKNOWLEDGE);

system.in.read()方法的作用是从键盘读出一个字符，然后返回它的Unicode码。
用System.in.read()时，我们在键盘上按下的任何一个键都会被当做是输入值，包括Enter键也会被当做是一个值！当我们按下Enter的时候，实际上发送两个键值：一个回车\t（13），一个是换行\n（10）


影响ActiveMQ性能的几个重要因素
Queue
1、Send/dispatch Async 影响非常大
     同步异步的发送和投递，都非常影响吞吐量。另外，SystemUsage和PFC流控对同步发送有直接影响。
2、Not transacted 去掉了记录redo日志
3、Auto_ACK/Optim_ACK 优化确认
     减少交互次数
4、Non-persistence 持久化消息，跟下面几点有关
    持久化和非持久化，也是数量级的影响，毕竟为了提高可靠性，使用数据库或文件来存消息，开销非常大。
5、pendingQueuePolicy/vmQueueCursor 决定了消息存储+发送模式，影响很大
    内存最快，文件和jdbc方式更安全，但是非常慢。。。
6、producerFlowControl/memoryLimit  可能会直接block掉producer
      vmCursor+非持久时，直接变成一个内存MQ，为了不爆掉jvm，在消息积压到指定数量的时候，PFC会阻止生产消息。
7、fast/slow consumer      决定了消息处理模式
     跟上面几点有关系。
8、在connection或connectionFactory上关闭掉 copyMessageOnSend
<!--StartFragment -->
根据JMS规范，消息是不可变的。send的时候，会自动的添加一些属性。有时候，可能会重用，或者多线程处理。为了不影响消息的不可变性，发送的时候，先复制一份，这样，发送时处理的消息对象和你的代码持有的消息对象，是两个不同对象了。相互之间就不会互相影响了。
一般情况下，这个选项可以关闭，从而获得一定的性能提升。
9、consumer端，获取消息时候的prefetchSize设置。 一定范围情况下，一次预获取越大，总体性能越好。


观察者：
public class MyObservable extends Observable {
    private String data;
    public void changeValue(String fValue) {
        data = fValue;
        setChanged();
    }
}
public class ObserverTest {
	public static void main(String[] args) {
        MyObservable myOservable = new MyObservable();
        myOservable.addObserver(new Observer() {
            //注册匿名内部类Observer，当数据改变时将通知该类的update方法
            public void update(Observable o, Object arg) {
                System.out.println("This value has been changed to " + (String) arg);
            }
        });
        String sValue = "Hello Msg";
        myOservable.changeValue(sValue);
        myOservable.notifyObservers(sValue + "!");//数据的改变由observable主动通知给Observer。
    }
}

进程查看的命令是ps和top
进程调度的命令有at，crontab，batch，kill，nohup。
batch命令用于在指定时间，当系统不繁忙时执行任务，用法与at相似

进程是cpu资源分配的最小单位，线程是cpu调度的最小单位。


ssh-keygen -t rsa -C "llhcc4137@sina.com"
git init
git add README.md
git commit -m "first commit"
git remote add origin https://github.com/llhcc/cxf.git
git push -u origin master

git 解决错误error: RPC failed; result=56, HTTP code = 200
git config --global http.postBuffer 24288000
git config --list
错误：
! [rejected]        master -> master (fetch first)

Git中从远程的分支获取最新的版本到本地有这样2个命令：
1. git fetch：相当于是从远程获取最新版本到本地，不会自动merge
    
Git fetch origin master
git log -p master..origin/master
git merge origin/master

    以上命令的含义：
   首先从远程的origin的master主分支下载最新的版本到origin/master分支上
   然后比较本地的master分支和origin/master分支的差别
   最后进行合并
   上述过程其实可以用以下更清晰的方式来进行：
 git fetch origin master:tmp
git diff tmp 
git merge tmp

    从远程获取最新的版本到本地的test分支上
   之后再进行比较合并
2. git pull：相当于是从远程获取最新版本并merge到本地
 git pull origin master

上述命令其实相当于git fetch 和 git merge
在实际使用中，git fetch更安全一些
因为在merge前，我们可以查看更新情况，然后再决定是否合并结束

Eclipse安装git
git
http://download.eclipse.org/egit/updates/



</2017-5-12>
